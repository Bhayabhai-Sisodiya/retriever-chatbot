#!/usr/bin/env python3
"""
LangGraph Agent with MCP Tools Integration

This module creates a LangGraph agent that uses MCP (Model Context Protocol) 
to access call transcription tools through the MCP server.
"""

import os
import asyncio
import sys
import logging
from typing import Dict, Any, List
from dotenv import load_dotenv

# Configure logging to suppress verbose INFO logs
logging.getLogger("mcp").setLevel(logging.WARNING)
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("httpcore").setLevel(logging.WARNING)
logging.getLogger("qdrant_client").setLevel(logging.WARNING)
logging.getLogger("urllib3").setLevel(logging.WARNING)
logging.getLogger("fastmcp").setLevel(logging.WARNING)
logging.getLogger("mcp.server").setLevel(logging.WARNING)
logging.getLogger("server").setLevel(logging.WARNING)

# Set root logger to WARNING to suppress all INFO logs
logging.basicConfig(level=logging.WARNING)

from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from langchain_mcp_adapters.tools import load_mcp_tools
from langgraph.prebuilt import create_react_agent
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage

# Load environment variables
load_dotenv()


class MCPLangGraphAgent:
    """LangGraph agent with MCP tools integration"""

    def __init__(self, server_script_path: str = "src/mcp_server.py"):
        """
        Initialize the MCP LangGraph agent

        Args:
            server_script_path: Path to the MCP server script
        """
        self.server_script_path = server_script_path
        self.llm = None
        self._llm_initialized = False

    def _initialize_llm(self):
        """Initialize the LLM once"""
        if not self._llm_initialized:
            self.llm = ChatOpenAI(
                model="gpt-3.5-turbo",
                api_key=os.getenv("OPENAI_API_KEY"),
                temperature=0.0
            )
            self._llm_initialized = True

    async def chat(self, message: str) -> str:
        """
        Chat with the agent using MCP tools (creates fresh connection each time)

        Args:
            message: User message

        Returns:
            Agent response
        """
        # Initialize LLM if not done
        self._initialize_llm()

        try:
            # Create server parameters for stdio connection
            server_params = StdioServerParameters(
                command="python",
                args=[self.server_script_path],
                env=None
            )

            # Use fresh connection for each chat (more reliable)
            async with stdio_client(server_params) as (read, write):
                async with ClientSession(read, write) as session:
                    # Initialize the connection
                    await session.initialize()

                    # Get tools
                    tools = await load_mcp_tools(session)

                    # Create agent with reused LLM
                    agent = create_react_agent(self.llm, tools)

                    # Create system message with instructions
                    system_message = SystemMessage(content="""You are a helpful assistant for call transcription processing and analysis.

You have access to tools for working with call transcripts:
1. ingest_transcription - Process and store new transcription files
2. get_last_transcript_summary - Get the most recent transcript
3. get_transcript_by_filename - Get specific transcript by filename
4. list_all_transcripts - List all available transcripts
5. answer_question_from_transcripts - Answer questions using RAG on transcripts

Guidelines:
- Use ingest_transcription when users want to process/load/import new files
- Use get_last_transcript_summary for "last", "latest", "most recent" requests
- Use get_transcript_by_filename when a specific filename is mentioned
- Use list_all_transcripts when users want to see all available transcripts
- Use answer_question_from_transcripts for specific questions about transcript content
- Always provide clear, helpful responses with relevant information

Be concise but informative in your responses.""")

                    # Run the agent
                    agent_response = await agent.ainvoke({
                        "messages": [
                            system_message,
                            HumanMessage(content=message)
                        ]
                    })

                    # Extract the final response
                    if agent_response and "messages" in agent_response:
                        last_message = agent_response["messages"][-1]
                        return last_message.content
                    else:
                        return "‚ùå No response from agent"

        except Exception as e:
            print(f"‚ùå Error processing message: {e}")
            import traceback
            traceback.print_exc()
            return f"‚ùå Error processing message: {e}"

async def interactive_chat():
    """Interactive chat interface"""
    agent = MCPLangGraphAgent()
    
    print("\n" + "="*60)
    print("ü§ñ MCP LangGraph Agent - Interactive Chat")
    print("="*60)
    print("You can ask me to:")
    print("  ‚Ä¢ Process transcription files: 'ingest the file at /path/to/file.txt'")
    print("  ‚Ä¢ Get recent transcripts: 'show me the last call'")
    print("  ‚Ä¢ Find specific transcripts: 'get transcript named demo_call.txt'")
    print("  ‚Ä¢ List all transcripts: 'list all available calls'")
    print("  ‚Ä¢ Answer questions: 'what was discussed about pricing?'")
    print("  ‚Ä¢ Type 'quit' to exit")
    print("="*60)
    
    try:
        while True:
            user_input = input("\nüó£Ô∏è You: ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'q']:
                break
            
            if not user_input:
                continue
            
            print("ü§ñ Processing... ‚ú® ", end="", flush=True)
            response = await agent.chat(user_input)
            print(response)
            
    except KeyboardInterrupt:
        print("\nüëã Goodbye!")
    except Exception as e:
        print(f"‚ùå Error: {e}")
    
    finally:
        pass  # No cleanup needed - connections are closed automatically


def main():
    """Main function"""
    asyncio.run(interactive_chat())


if __name__ == "__main__":
    main()
